{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darknet import Darknet\n",
    "import dataset\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from utils import *\n",
    "import os\n",
    "\n",
    "datacfg = 'cfg/voc.data'\n",
    "cfgfile = 'cfg/yolo-voc.cfg'\n",
    "weightfile = '/home/rliu/000510.weights'\n",
    "outfile = 'comp4_det_test_'\n",
    "\n",
    "\n",
    "options = read_data_cfg(datacfg)\n",
    "valid_images = options['valid']\n",
    "name_list = options['names']\n",
    "prefix = 'results'\n",
    "names = load_class_names(name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backup': 'backup',\n",
       " 'gpus': '0,1',\n",
       " 'names': 'data/an.names',\n",
       " 'num_workers': '10',\n",
       " 'train': 'data/voc_train.txt',\n",
       " 'valid': 'data/voc_train.txt'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer     filters    size              input                output\n",
      "    0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32\n",
      "    1 max          2 x 2 / 2   416 x 416 x  32   ->   208 x 208 x  32\n",
      "    2 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64\n",
      "    3 max          2 x 2 / 2   208 x 208 x  64   ->   104 x 104 x  64\n",
      "    4 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128\n",
      "    5 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64\n",
      "    6 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128\n",
      "    7 max          2 x 2 / 2   104 x 104 x 128   ->    52 x  52 x 128\n",
      "    8 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "    9 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128\n",
      "   10 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256\n",
      "   11 max          2 x 2 / 2    52 x  52 x 256   ->    26 x  26 x 256\n",
      "   12 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   13 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   14 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   15 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256\n",
      "   16 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512\n",
      "   17 max          2 x 2 / 2    26 x  26 x 512   ->    13 x  13 x 512\n",
      "   18 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   19 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   20 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   21 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512\n",
      "   22 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024\n",
      "   23 conv   1024  3 x 3 / 1    13 x  13 x1024   ->    13 x  13 x1024\n",
      "   24 conv   1024  3 x 3 / 1    13 x  13 x1024   ->    13 x  13 x1024\n",
      "   25 route  16\n",
      "   26 conv     64  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x  64\n",
      "   27 reorg              / 2    26 x  26 x  64   ->    13 x  13 x 256\n",
      "   28 route  27 24\n",
      "   29 conv   1024  3 x 3 / 1    13 x  13 x1280   ->    13 x  13 x1024\n",
      "   30 conv    125  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 125\n",
      "   31 detection\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (3) must match the existing size (864) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-38c7e794340a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfgfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweightfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rliu/github/defect_classifier/yolo2/darknet.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, weightfile)\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch_normalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_normalize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_normalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_conv_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;31m#                     print(model[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rliu/github/defect_classifier/yolo2/cfg.py\u001b[0m in \u001b[0;36mload_conv_bn\u001b[0;34m(buf, start, conv_model, bn_model)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mbn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnum_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mbn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnum_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m   \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mconv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnum_w\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;31m#     conv_model.weight.data.copy_(torch.reshape(torch.from_numpy(buf[start:start+num_w]),(conv_model.weight.shape[0],conv_model.weight.shape[1], conv_model.weight.shape[2],conv_model.weight.shape[3]))); start=start + num_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (3) must match the existing size (864) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(valid_images) as fp:\n",
    "    tmp_files = fp.readlines()\n",
    "    valid_files = [item.rstrip() for item in tmp_files]\n",
    "\n",
    "m = Darknet(cfgfile)\n",
    "m.print_network()\n",
    "m.load_weights(weightfile)\n",
    "m.cuda()\n",
    "m.eval()\n",
    "\n",
    "valid_dataset = dataset.listDataset(valid_images, shape=(m.width, m.height),\n",
    "                   shuffle=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ]))\n",
    "valid_batchsize = 16\n",
    "assert(valid_batchsize > 1)\n",
    "\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True}\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=valid_batchsize, shuffle=False, **kwargs) \n",
    "\n",
    "fps = [0]*m.num_classes\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "for i in range(m.num_classes):\n",
    "    buf = '%s/%s%s.txt' % (prefix, outfile, names[i])\n",
    "    fps[i] = open(buf, 'w')\n",
    "\n",
    "lineId = -1\n",
    "\n",
    "conf_thresh = 0\n",
    "nms_thresh = 0.30\n",
    "for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "    data = data.cuda()\n",
    "    data = Variable(data, volatile = True)\n",
    "    output = m(data).data\n",
    "    batch_boxes = get_region_boxes(output, conf_thresh, m.num_classes, m.anchors, m.num_anchors, 0, 1)\n",
    "    for i in range(output.size(0)):\n",
    "        lineId = lineId + 1\n",
    "        fileId = os.path.basename(valid_files[lineId]).split('.')[0]\n",
    "        width, height = get_image_size(valid_files[lineId])\n",
    "        print(valid_files[lineId])\n",
    "        boxes = batch_boxes[i]\n",
    "        boxes = nms(boxes, nms_thresh)\n",
    "        for box in boxes:\n",
    "            x1 = (box[0] - box[2]/2.0) * width\n",
    "            y1 = (box[1] - box[3]/2.0) * height\n",
    "            x2 = (box[0] + box[2]/2.0) * width\n",
    "            y2 = (box[1] + box[3]/2.0) * height\n",
    "\n",
    "            det_conf = box[4]\n",
    "            for j in range((len(box)-5)/2):\n",
    "                cls_conf = box[5+2*j]\n",
    "                cls_id = box[6+2*j]\n",
    "                prob =det_conf * cls_conf\n",
    "                fps[cls_id].write('%s %f %f %f %f %f\\n' % (fileId, prob, x1, y1, x2, y2))\n",
    "\n",
    "for i in range(m.num_classes):\n",
    "    fps[i].close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9a40b379906c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
